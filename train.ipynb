{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Notebook\n",
    "---\n",
    "\n",
    "## Load the Data\n",
    "Use one of the cells below to either parse the data from the file structure or load the data from the saved numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "\n",
    "image_files, labels = utils.getprocessedfilelist()\n",
    "np.save(os.path.join('data_road', 'images.npy'), image_files)\n",
    "np.save(os.path.join('data_road', 'labels.npy'), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "\n",
    "# use this cell to load the numpy data from disk so that you\n",
    "# don't always have to parse the file structure\n",
    "image_files = np.load(os.path.join('data_road', 'images.npy'))\n",
    "labels = np.load(os.path.join('data_road', 'labels.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Input Pipeline\n",
    "---\n",
    "Create an input pipeline from the loaded arrays. Split the pipeline into train and val datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'data_road\\\\processed_images\\\\umm_000000_0.jpg', array([160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160.]))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "data = tf.data.Dataset.from_tensor_slices((image_files, labels))\n",
    "\n",
    "@tf.function\n",
    "def readimage(image_file, label):\n",
    "    image = tf.io.decode_jpeg(tf.io.read_file(image_file)) / 255\n",
    "    return image, label\n",
    "\n",
    "batch_size = 32\n",
    "shuffle_buffer_size = len(labels)\n",
    "train_ds_size = 20000\n",
    "val_ds_size = 5000\n",
    "\n",
    "shuffled_ds = data.shuffle(shuffle_buffer_size).map(readimage, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = shuffled_ds.take(train_ds_size).batch(batch_size)\n",
    "val_ds = shuffled_ds.skip(train_ds_size).take(val_ds_size).batch(batch_size)\n",
    "\n",
    "iter_ = shuffled_ds.as_numpy_iterator()\n",
    "a = iter_.next()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Model\n",
    "---\n",
    "\n",
    "Create the model and add callbacks for writing checkpoints and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Conv1D, LSTM, MaxPool2D, Flatten, InputLayer, Reshape, TimeDistributed\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        InputLayer(input_shape=(160,600,3)),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        MaxPool2D(),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        MaxPool2D(),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        MaxPool2D(),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        TimeDistributed(LSTM(64, return_sequences=True)),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        TimeDistributed(LSTM(64, return_sequences=True)),\n",
    "        Conv2D(64, (5,1), strides=(4,1), activation='relu'),\n",
    "        Conv2D(1, (4,1), activation='relu'),\n",
    "        Flatten(),\n",
    "        Dense(600, activation='linear')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join('ckpt','training-1'), \n",
    "        save_weights_only=True)\n",
    "]\n",
    "\n",
    "model = create_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "625/625 [==============================] - 192s 287ms/step - loss: 19883.3926 - accuracy: 0.0017 - val_loss: 19863.1465 - val_accuracy: 6.0000e-04\n",
      "Epoch 2/80\n",
      "625/625 [==============================] - 167s 267ms/step - loss: 19832.4531 - accuracy: 4.0000e-04 - val_loss: 19800.1113 - val_accuracy: 6.0000e-04\n",
      "Epoch 3/80\n",
      "625/625 [==============================] - 162s 259ms/step - loss: 19640.5293 - accuracy: 4.0000e-04 - val_loss: 19455.3242 - val_accuracy: 0.0010\n",
      "Epoch 4/80\n",
      "625/625 [==============================] - 158s 253ms/step - loss: 19448.8340 - accuracy: 0.0040 - val_loss: 19468.6992 - val_accuracy: 0.0112\n",
      "Epoch 5/80\n",
      "625/625 [==============================] - 159s 255ms/step - loss: 19281.8730 - accuracy: 0.3920 - val_loss: 19094.4199 - val_accuracy: 0.6608\n",
      "Epoch 6/80\n",
      "625/625 [==============================] - 158s 253ms/step - loss: 19147.3047 - accuracy: 0.2991 - val_loss: 19074.7344 - val_accuracy: 0.0122\n",
      "Epoch 7/80\n",
      "625/625 [==============================] - 162s 259ms/step - loss: 19087.1465 - accuracy: 0.0102 - val_loss: 18904.3730 - val_accuracy: 0.0102\n",
      "Epoch 8/80\n",
      "625/625 [==============================] - 163s 261ms/step - loss: 18834.7734 - accuracy: 0.0984 - val_loss: 18862.6602 - val_accuracy: 0.0116\n",
      "Epoch 9/80\n",
      "625/625 [==============================] - 165s 264ms/step - loss: 18524.4375 - accuracy: 0.0116 - val_loss: 18971.4902 - val_accuracy: 0.0122\n",
      "Epoch 10/80\n",
      "625/625 [==============================] - 172s 275ms/step - loss: 18495.7676 - accuracy: 0.0110 - val_loss: 18554.6387 - val_accuracy: 0.0130\n",
      "Epoch 11/80\n",
      "625/625 [==============================] - 172s 275ms/step - loss: 18421.6582 - accuracy: 0.0108 - val_loss: 18462.7090 - val_accuracy: 0.0118\n",
      "Epoch 12/80\n",
      "625/625 [==============================] - 173s 277ms/step - loss: 18153.0879 - accuracy: 0.0115 - val_loss: 17943.4102 - val_accuracy: 0.0142\n",
      "Epoch 13/80\n",
      "625/625 [==============================] - 172s 276ms/step - loss: 18104.5625 - accuracy: 0.0110 - val_loss: 17917.9238 - val_accuracy: 0.0116\n",
      "Epoch 14/80\n",
      "625/625 [==============================] - 173s 277ms/step - loss: 17977.3789 - accuracy: 0.0118 - val_loss: 17597.6328 - val_accuracy: 0.0110\n",
      "Epoch 15/80\n",
      "625/625 [==============================] - 173s 276ms/step - loss: 17742.8301 - accuracy: 0.0111 - val_loss: 17783.4727 - val_accuracy: 0.0110\n",
      "Epoch 16/80\n",
      "625/625 [==============================] - 166s 266ms/step - loss: 17685.9668 - accuracy: 0.0110 - val_loss: 17460.2793 - val_accuracy: 0.0118\n",
      "Epoch 17/80\n",
      "625/625 [==============================] - 166s 266ms/step - loss: 17451.4375 - accuracy: 0.0098 - val_loss: 17280.3418 - val_accuracy: 0.0122\n",
      "Epoch 18/80\n",
      "625/625 [==============================] - 166s 265ms/step - loss: 17320.1543 - accuracy: 0.0106 - val_loss: 17087.9766 - val_accuracy: 0.0098\n",
      "Epoch 19/80\n",
      "625/625 [==============================] - 169s 270ms/step - loss: 17175.3105 - accuracy: 0.0103 - val_loss: 17202.2168 - val_accuracy: 0.0116\n",
      "Epoch 20/80\n",
      "625/625 [==============================] - 164s 262ms/step - loss: 16962.4590 - accuracy: 0.0110 - val_loss: 16773.8594 - val_accuracy: 0.0116\n",
      "Epoch 21/80\n",
      "625/625 [==============================] - 155s 248ms/step - loss: 16915.8906 - accuracy: 0.0105 - val_loss: 16625.5352 - val_accuracy: 0.0108\n",
      "Epoch 22/80\n",
      "625/625 [==============================] - 155s 248ms/step - loss: 16689.7617 - accuracy: 0.0106 - val_loss: 16634.5625 - val_accuracy: 0.0098\n",
      "Epoch 23/80\n",
      "625/625 [==============================] - 158s 254ms/step - loss: 16631.0664 - accuracy: 0.0110 - val_loss: 16473.4082 - val_accuracy: 0.0084\n",
      "Epoch 24/80\n",
      "625/625 [==============================] - 162s 259ms/step - loss: 16416.0488 - accuracy: 0.0109 - val_loss: 16556.2480 - val_accuracy: 0.0124\n",
      "Epoch 25/80\n",
      "625/625 [==============================] - 165s 264ms/step - loss: 16179.0146 - accuracy: 0.0109 - val_loss: 16038.5557 - val_accuracy: 0.0142\n",
      "Epoch 26/80\n",
      "625/625 [==============================] - 160s 256ms/step - loss: 16064.1152 - accuracy: 0.0110 - val_loss: 15843.4404 - val_accuracy: 0.0098\n",
      "Epoch 27/80\n",
      "625/625 [==============================] - 161s 257ms/step - loss: 15926.7324 - accuracy: 0.0106 - val_loss: 15752.6992 - val_accuracy: 0.0110\n",
      "Epoch 28/80\n",
      "625/625 [==============================] - 162s 259ms/step - loss: 15797.7119 - accuracy: 0.0111 - val_loss: 15688.3906 - val_accuracy: 0.0116\n",
      "Epoch 29/80\n",
      "625/625 [==============================] - 167s 267ms/step - loss: 15693.6660 - accuracy: 0.0110 - val_loss: 15501.2803 - val_accuracy: 0.0114\n",
      "Epoch 30/80\n",
      "625/625 [==============================] - 168s 268ms/step - loss: 15439.6143 - accuracy: 0.0114 - val_loss: 15246.4766 - val_accuracy: 0.0094\n",
      "Epoch 31/80\n",
      "625/625 [==============================] - 168s 269ms/step - loss: 15424.5195 - accuracy: 0.0104 - val_loss: 15245.1729 - val_accuracy: 0.0124\n",
      "Epoch 32/80\n",
      "625/625 [==============================] - 166s 266ms/step - loss: 15336.8301 - accuracy: 0.0106 - val_loss: 14981.0879 - val_accuracy: 0.0120\n",
      "Epoch 33/80\n",
      "625/625 [==============================] - 168s 269ms/step - loss: 15085.5059 - accuracy: 0.0106 - val_loss: 14850.8799 - val_accuracy: 0.0106\n",
      "Epoch 34/80\n",
      "625/625 [==============================] - 167s 268ms/step - loss: 14958.5420 - accuracy: 0.0104 - val_loss: 14770.3076 - val_accuracy: 0.0108\n",
      "Epoch 35/80\n",
      "625/625 [==============================] - 166s 266ms/step - loss: 14903.5361 - accuracy: 0.0113 - val_loss: 14869.4834 - val_accuracy: 0.0114\n",
      "Epoch 36/80\n",
      "625/625 [==============================] - 172s 275ms/step - loss: 14684.1074 - accuracy: 0.0111 - val_loss: 14468.0283 - val_accuracy: 0.0088\n",
      "Epoch 37/80\n",
      "625/625 [==============================] - 176s 281ms/step - loss: 14560.2959 - accuracy: 0.0107 - val_loss: 14644.3057 - val_accuracy: 0.0114\n",
      "Epoch 38/80\n",
      "625/625 [==============================] - 177s 283ms/step - loss: 14497.5361 - accuracy: 0.0105 - val_loss: 14499.1035 - val_accuracy: 0.0144\n",
      "Epoch 39/80\n",
      "625/625 [==============================] - 177s 283ms/step - loss: 14279.8125 - accuracy: 0.0103 - val_loss: 14099.0107 - val_accuracy: 0.0112\n",
      "Epoch 40/80\n",
      "625/625 [==============================] - 169s 270ms/step - loss: 14087.2451 - accuracy: 0.0105 - val_loss: 14143.8467 - val_accuracy: 0.0116\n",
      "Epoch 41/80\n",
      "625/625 [==============================] - 167s 267ms/step - loss: 14027.0107 - accuracy: 0.0117 - val_loss: 13836.7822 - val_accuracy: 0.0118\n",
      "Epoch 42/80\n",
      "625/625 [==============================] - 168s 268ms/step - loss: 13896.8125 - accuracy: 0.0118 - val_loss: 13724.2852 - val_accuracy: 0.0090\n",
      "Epoch 43/80\n",
      "625/625 [==============================] - 167s 267ms/step - loss: 13746.5264 - accuracy: 0.0110 - val_loss: 13431.9121 - val_accuracy: 0.0098\n",
      "Epoch 44/80\n",
      "625/625 [==============================] - 168s 269ms/step - loss: 13612.2881 - accuracy: 0.0114 - val_loss: 13581.2227 - val_accuracy: 0.0092\n",
      "Epoch 45/80\n",
      "625/625 [==============================] - 169s 270ms/step - loss: 13424.4990 - accuracy: 0.0108 - val_loss: 13467.4307 - val_accuracy: 0.0110\n",
      "Epoch 46/80\n",
      "625/625 [==============================] - 168s 268ms/step - loss: 13352.1182 - accuracy: 0.0115 - val_loss: 13171.3779 - val_accuracy: 0.0104\n",
      "Epoch 47/80\n",
      "625/625 [==============================] - 167s 267ms/step - loss: 13171.0176 - accuracy: 0.0123 - val_loss: 13177.2354 - val_accuracy: 0.0140\n",
      "Epoch 48/80\n",
      "625/625 [==============================] - 165s 264ms/step - loss: 13145.4209 - accuracy: 0.0104 - val_loss: 12704.8203 - val_accuracy: 0.0096\n",
      "Epoch 49/80\n",
      "625/625 [==============================] - 165s 264ms/step - loss: 13017.7812 - accuracy: 0.0110 - val_loss: 12867.3418 - val_accuracy: 0.0126\n",
      "Epoch 50/80\n",
      "387/625 [=================>............] - ETA: 55s - loss: 12895.5908 - accuracy: 0.0115"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model.fit(x=train_ds, batch_size=batch_size, epochs=80, callbacks=callbacks, validation_data=val_ds, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "accf17c8cddd7598f858c901abd76bdd6a40d95d35cb16ec1025b995904db703"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
