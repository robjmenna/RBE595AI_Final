{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "\n",
    "image_files, labels = utils.getprocessedfilelist()\n",
    "np.save(os.path.join('data_road', 'images.npy'), image_files)\n",
    "np.save(os.path.join('data_road', 'labels.npy'), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "\n",
    "# use this cell to load the numpy data from disk so that you\n",
    "# don't always have to parse the file structure\n",
    "image_files = np.load(os.path.join('data_road', 'images.npy'))\n",
    "labels = np.load(os.path.join('data_road', 'labels.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'data_road\\\\processed_images\\\\umm_000000_0.jpg', array([160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160., 160., 160., 160., 160., 160.,\n",
      "       160., 160., 160., 160., 160., 160.]))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "data = tf.data.Dataset.from_tensor_slices((image_files, labels))\n",
    "a = data.as_numpy_iterator()\n",
    "b = a.next()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[[0.13333334, 0.16078432, 0.2       ],\n",
      "        [0.26666668, 0.2627451 , 0.25490198],\n",
      "        [0.7058824 , 0.62352943, 0.5411765 ],\n",
      "        ...,\n",
      "        [1.        , 0.99607843, 0.972549  ],\n",
      "        [1.        , 0.95686275, 0.9137255 ],\n",
      "        [0.80784315, 0.7058824 , 0.654902  ]],\n",
      "\n",
      "       [[0.20392157, 0.24705882, 0.27058825],\n",
      "        [0.31764707, 0.32156864, 0.3019608 ],\n",
      "        [0.7058824 , 0.627451  , 0.53333336],\n",
      "        ...,\n",
      "        [1.        , 0.99607843, 0.972549  ],\n",
      "        [1.        , 0.96862745, 0.92941177],\n",
      "        [0.85490197, 0.76862746, 0.7176471 ]],\n",
      "\n",
      "       [[0.21568628, 0.3019608 , 0.2901961 ],\n",
      "        [0.3647059 , 0.39607844, 0.3529412 ],\n",
      "        [0.7372549 , 0.67058825, 0.56078434],\n",
      "        ...,\n",
      "        [1.        , 1.        , 0.972549  ],\n",
      "        [1.        , 0.99215686, 0.9529412 ],\n",
      "        [0.87058824, 0.8156863 , 0.7647059 ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.9843137 , 1.        , 1.        ],\n",
      "        [0.9882353 , 1.        , 1.        ],\n",
      "        [1.        , 1.        , 0.99215686],\n",
      "        ...,\n",
      "        [0.80784315, 0.8235294 , 0.8352941 ],\n",
      "        [0.6666667 , 0.68235296, 0.6862745 ],\n",
      "        [0.58431375, 0.6       , 0.59607846]],\n",
      "\n",
      "       [[1.        , 1.        , 1.        ],\n",
      "        [1.        , 1.        , 0.99215686],\n",
      "        [1.        , 1.        , 0.98039216],\n",
      "        ...,\n",
      "        [0.9882353 , 1.        , 1.        ],\n",
      "        [0.93333334, 0.95686275, 0.9411765 ],\n",
      "        [0.88235295, 0.9098039 , 0.88235295]],\n",
      "\n",
      "       [[1.        , 0.99607843, 1.        ],\n",
      "        [1.        , 0.99607843, 0.9843137 ],\n",
      "        [1.        , 1.        , 0.972549  ],\n",
      "        ...,\n",
      "        [0.9882353 , 1.        , 0.9843137 ],\n",
      "        [0.9843137 , 1.        , 0.9843137 ],\n",
      "        [0.972549  , 1.        , 0.9764706 ]]], dtype=float32), array([ 99.,  99.,  99.,  98.,  98.,  98.,  97.,  97.,  97.,  97.,  96.,\n",
      "        96.,  96.,  95.,  95.,  95.,  95.,  94.,  94.,  94.,  93.,  93.,\n",
      "        93.,  93.,  92.,  92.,  92.,  91.,  91.,  91.,  91.,  90.,  90.,\n",
      "        90.,  90.,  89.,  89.,  89.,  88.,  88.,  88.,  88.,  87.,  87.,\n",
      "        87.,  86.,  86.,  86.,  86.,  85.,  85.,  85.,  84.,  84.,  84.,\n",
      "        84.,  83.,  83.,  83.,  82.,  82.,  82.,  82.,  81.,  81.,  81.,\n",
      "        80.,  80.,  80.,  80.,  79.,  79.,  79.,  78.,  78.,  78.,  78.,\n",
      "        77.,  77.,  77.,  77.,  76.,  76.,  76.,  75.,  75.,  75.,  75.,\n",
      "        74.,  74.,  74.,  73.,  73.,  73.,  73.,  72.,  72.,  72.,  71.,\n",
      "        71.,  71.,  71.,  70.,  70.,  70.,  69.,  69.,  69.,  69.,  68.,\n",
      "        68.,  68.,  67.,  67.,  67.,  67.,  66.,  66.,  66.,  65.,  65.,\n",
      "        65.,  65.,  64.,  64.,  64.,  63.,  63.,  63.,  63.,  62.,  62.,\n",
      "        62.,  62.,  61.,  61.,  61.,  60.,  60.,  60.,  60.,  59.,  59.,\n",
      "        59.,  58.,  58.,  58.,  58.,  57.,  57.,  57.,  56.,  56.,  56.,\n",
      "        56.,  55.,  55.,  55.,  55.,  54.,  54.,  54.,  54.,  53.,  53.,\n",
      "        53.,  53.,  52.,  52.,  52.,  52.,  52.,  51.,  51.,  51.,  51.,\n",
      "        50.,  50.,  50.,  50.,  49.,  49.,  49.,  49.,  49.,  48.,  48.,\n",
      "        48.,  48.,  47.,  47.,  47.,  47.,  47.,  46.,  46.,  46.,  46.,\n",
      "        45.,  45.,  45.,  45.,  44.,  44.,  44.,  44.,  44.,  43.,  43.,\n",
      "        43.,  43.,  42.,  42.,  42.,  42.,  41.,  41.,  41.,  41.,  41.,\n",
      "        40.,  40.,  40.,  40.,  39.,  39.,  39.,  39.,  38.,  38.,  38.,\n",
      "        38.,  38.,  37.,  37.,  37.,  37.,  36.,  36.,  36.,  36.,  36.,\n",
      "        35.,  35.,  35.,  35.,  34.,  34.,  34.,  34.,  33.,  33.,  33.,\n",
      "        33.,  33.,  32.,  32.,  32.,  32.,  31.,  31.,  31.,  31.,  30.,\n",
      "        30.,  30.,  30.,  30.,  29.,  29.,  29.,  29.,  29.,  29.,  28.,\n",
      "        28.,  28.,  28.,  28.,  28.,  27.,  27.,  27.,  27.,  27.,  27.,\n",
      "        26.,  26.,  26.,  26.,  26.,  25.,  25.,  25.,  25.,  25.,  25.,\n",
      "        24.,  24.,  24.,  24.,  24.,  24.,  23.,  23.,  23.,  23.,  23.,\n",
      "        23.,  23.,  23.,  23.,  23.,  23.,  23.,  23.,  23.,  23.,  23.,\n",
      "        23.,  23.,  23.,  23.,  23.,  23.,  23.,  23.,  23.,  23.,  23.,\n",
      "        23.,  23.,  23.,  23.,  23.,  23.,  23.,  23.,  23.,  23.,  23.,\n",
      "        23.,  23.,  23.,  23.,  23.,  23.,  23.,  23.,  23.,  23.,  23.,\n",
      "        30.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,\n",
      "        36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,\n",
      "        36.,  36.,  36.,  36.,  36.,  23.,  23.,  23.,  23.,  23.,  23.,\n",
      "        23.,  36.,  36.,  37.,  38.,  38.,  39.,  40.,  40.,  41.,  42.,\n",
      "        42.,  43.,  43.,  44.,  44.,  45.,  45.,  46.,  46.,  47.,  47.,\n",
      "        48.,  48.,  49.,  49.,  50.,  50.,  51.,  51.,  52.,  52.,  53.,\n",
      "        53.,  54.,  54.,  55.,  55.,  56.,  56.,  57.,  57.,  58.,  58.,\n",
      "        59.,  59.,  59.,  60.,  60.,  60.,  61.,  61.,  61.,  62.,  62.,\n",
      "        62.,  62.,  63.,  63.,  63.,  64.,  64.,  64.,  65.,  65.,  65.,\n",
      "        66.,  66.,  66.,  67.,  67.,  67.,  68.,  68.,  68.,  69.,  69.,\n",
      "        69.,  70.,  70.,  70.,  70.,  71.,  71.,  71.,  72.,  72.,  72.,\n",
      "        73.,  73.,  73.,  74.,  74.,  74.,  75.,  75.,  75.,  76.,  76.,\n",
      "        76.,  77.,  77.,  77.,  78.,  78.,  78.,  79.,  79.,  79.,  79.,\n",
      "        80.,  80.,  80.,  81.,  81.,  81.,  82.,  82.,  82.,  83.,  83.,\n",
      "        83.,  84.,  84.,  84.,  85.,  85.,  85.,  86.,  86.,  86.,  87.,\n",
      "        87.,  87.,  88.,  88.,  88.,  89.,  89.,  89.,  90.,  90.,  90.,\n",
      "        90.,  91.,  91.,  91.,  92.,  92.,  92.,  93.,  93.,  93.,  94.,\n",
      "        94.,  94.,  95.,  95.,  95.,  96.,  96.,  96.,  97.,  97.,  97.,\n",
      "        98.,  98.,  98.,  99.,  99.,  99., 100., 100., 100., 101., 101.,\n",
      "       101., 101., 102., 102., 102., 103., 103., 103., 104., 104., 104.,\n",
      "       105., 105., 105., 106., 106., 106., 107., 107., 107., 108., 108.,\n",
      "       108., 109., 109., 109., 110., 110., 110., 111., 111., 111., 112.,\n",
      "       112., 112., 112., 113., 113., 113.]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "@tf.function\n",
    "def readimage(image_file, label):\n",
    "    image = tf.io.decode_jpeg(tf.io.read_file(image_file)) / 255\n",
    "    return image, label\n",
    "\n",
    "shuffled_ds = data.shuffle(len(labels)).map(readimage, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "iter_ = shuffled_ds.as_numpy_iterator()\n",
    "a = iter_.next()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Conv1D, LSTM, MaxPool2D, Flatten, InputLayer, Reshape, TimeDistributed\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        InputLayer(input_shape=(160,600,3)),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        MaxPool2D(),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        MaxPool2D(),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        MaxPool2D(),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        TimeDistributed(LSTM(64, return_sequences=True)),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        TimeDistributed(LSTM(64, return_sequences=True)),\n",
    "        Conv2D(64, (5,1), strides=(4,1), activation='relu'),\n",
    "        Conv2D(1, (4,1), activation='relu'),\n",
    "        Flatten(),\n",
    "        Dense(600, activation='linear')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join('ckpt','training-1'), \n",
    "        save_weights_only=True)\n",
    "]\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = shuffled_ds.take(20000).batch(batch_size)\n",
    "val_ds = shuffled_ds.skip(20000).take(5000).batch(batch_size)\n",
    "model = create_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "625/625 [==============================] - 192s 287ms/step - loss: 19883.3926 - accuracy: 0.0017 - val_loss: 19863.1465 - val_accuracy: 6.0000e-04\n",
      "Epoch 2/80\n",
      "625/625 [==============================] - 167s 267ms/step - loss: 19832.4531 - accuracy: 4.0000e-04 - val_loss: 19800.1113 - val_accuracy: 6.0000e-04\n",
      "Epoch 3/80\n",
      "625/625 [==============================] - 162s 259ms/step - loss: 19640.5293 - accuracy: 4.0000e-04 - val_loss: 19455.3242 - val_accuracy: 0.0010\n",
      "Epoch 4/80\n",
      "625/625 [==============================] - 158s 253ms/step - loss: 19448.8340 - accuracy: 0.0040 - val_loss: 19468.6992 - val_accuracy: 0.0112\n",
      "Epoch 5/80\n",
      "625/625 [==============================] - 159s 255ms/step - loss: 19281.8730 - accuracy: 0.3920 - val_loss: 19094.4199 - val_accuracy: 0.6608\n",
      "Epoch 6/80\n",
      "625/625 [==============================] - 158s 253ms/step - loss: 19147.3047 - accuracy: 0.2991 - val_loss: 19074.7344 - val_accuracy: 0.0122\n",
      "Epoch 7/80\n",
      "625/625 [==============================] - 162s 259ms/step - loss: 19087.1465 - accuracy: 0.0102 - val_loss: 18904.3730 - val_accuracy: 0.0102\n",
      "Epoch 8/80\n",
      "625/625 [==============================] - 163s 261ms/step - loss: 18834.7734 - accuracy: 0.0984 - val_loss: 18862.6602 - val_accuracy: 0.0116\n",
      "Epoch 9/80\n",
      "625/625 [==============================] - 165s 264ms/step - loss: 18524.4375 - accuracy: 0.0116 - val_loss: 18971.4902 - val_accuracy: 0.0122\n",
      "Epoch 10/80\n",
      "625/625 [==============================] - 172s 275ms/step - loss: 18495.7676 - accuracy: 0.0110 - val_loss: 18554.6387 - val_accuracy: 0.0130\n",
      "Epoch 11/80\n",
      "625/625 [==============================] - 172s 275ms/step - loss: 18421.6582 - accuracy: 0.0108 - val_loss: 18462.7090 - val_accuracy: 0.0118\n",
      "Epoch 12/80\n",
      "625/625 [==============================] - 173s 277ms/step - loss: 18153.0879 - accuracy: 0.0115 - val_loss: 17943.4102 - val_accuracy: 0.0142\n",
      "Epoch 13/80\n",
      "625/625 [==============================] - 172s 276ms/step - loss: 18104.5625 - accuracy: 0.0110 - val_loss: 17917.9238 - val_accuracy: 0.0116\n",
      "Epoch 14/80\n",
      "625/625 [==============================] - 173s 277ms/step - loss: 17977.3789 - accuracy: 0.0118 - val_loss: 17597.6328 - val_accuracy: 0.0110\n",
      "Epoch 15/80\n",
      "625/625 [==============================] - 173s 276ms/step - loss: 17742.8301 - accuracy: 0.0111 - val_loss: 17783.4727 - val_accuracy: 0.0110\n",
      "Epoch 16/80\n",
      "625/625 [==============================] - 166s 266ms/step - loss: 17685.9668 - accuracy: 0.0110 - val_loss: 17460.2793 - val_accuracy: 0.0118\n",
      "Epoch 17/80\n",
      "625/625 [==============================] - 166s 266ms/step - loss: 17451.4375 - accuracy: 0.0098 - val_loss: 17280.3418 - val_accuracy: 0.0122\n",
      "Epoch 18/80\n",
      "625/625 [==============================] - 166s 265ms/step - loss: 17320.1543 - accuracy: 0.0106 - val_loss: 17087.9766 - val_accuracy: 0.0098\n",
      "Epoch 19/80\n",
      "625/625 [==============================] - 169s 270ms/step - loss: 17175.3105 - accuracy: 0.0103 - val_loss: 17202.2168 - val_accuracy: 0.0116\n",
      "Epoch 20/80\n",
      "625/625 [==============================] - 164s 262ms/step - loss: 16962.4590 - accuracy: 0.0110 - val_loss: 16773.8594 - val_accuracy: 0.0116\n",
      "Epoch 21/80\n",
      "625/625 [==============================] - 155s 248ms/step - loss: 16915.8906 - accuracy: 0.0105 - val_loss: 16625.5352 - val_accuracy: 0.0108\n",
      "Epoch 22/80\n",
      "625/625 [==============================] - 155s 248ms/step - loss: 16689.7617 - accuracy: 0.0106 - val_loss: 16634.5625 - val_accuracy: 0.0098\n",
      "Epoch 23/80\n",
      "625/625 [==============================] - 158s 254ms/step - loss: 16631.0664 - accuracy: 0.0110 - val_loss: 16473.4082 - val_accuracy: 0.0084\n",
      "Epoch 24/80\n",
      "625/625 [==============================] - 162s 259ms/step - loss: 16416.0488 - accuracy: 0.0109 - val_loss: 16556.2480 - val_accuracy: 0.0124\n",
      "Epoch 25/80\n",
      "625/625 [==============================] - 165s 264ms/step - loss: 16179.0146 - accuracy: 0.0109 - val_loss: 16038.5557 - val_accuracy: 0.0142\n",
      "Epoch 26/80\n",
      "625/625 [==============================] - 160s 256ms/step - loss: 16064.1152 - accuracy: 0.0110 - val_loss: 15843.4404 - val_accuracy: 0.0098\n",
      "Epoch 27/80\n",
      "625/625 [==============================] - 161s 257ms/step - loss: 15926.7324 - accuracy: 0.0106 - val_loss: 15752.6992 - val_accuracy: 0.0110\n",
      "Epoch 28/80\n",
      "625/625 [==============================] - 162s 259ms/step - loss: 15797.7119 - accuracy: 0.0111 - val_loss: 15688.3906 - val_accuracy: 0.0116\n",
      "Epoch 29/80\n",
      "625/625 [==============================] - 167s 267ms/step - loss: 15693.6660 - accuracy: 0.0110 - val_loss: 15501.2803 - val_accuracy: 0.0114\n",
      "Epoch 30/80\n",
      "625/625 [==============================] - 168s 268ms/step - loss: 15439.6143 - accuracy: 0.0114 - val_loss: 15246.4766 - val_accuracy: 0.0094\n",
      "Epoch 31/80\n",
      "625/625 [==============================] - 168s 269ms/step - loss: 15424.5195 - accuracy: 0.0104 - val_loss: 15245.1729 - val_accuracy: 0.0124\n",
      "Epoch 32/80\n",
      "625/625 [==============================] - 166s 266ms/step - loss: 15336.8301 - accuracy: 0.0106 - val_loss: 14981.0879 - val_accuracy: 0.0120\n",
      "Epoch 33/80\n",
      "625/625 [==============================] - 168s 269ms/step - loss: 15085.5059 - accuracy: 0.0106 - val_loss: 14850.8799 - val_accuracy: 0.0106\n",
      "Epoch 34/80\n",
      "625/625 [==============================] - 167s 268ms/step - loss: 14958.5420 - accuracy: 0.0104 - val_loss: 14770.3076 - val_accuracy: 0.0108\n",
      "Epoch 35/80\n",
      "625/625 [==============================] - 166s 266ms/step - loss: 14903.5361 - accuracy: 0.0113 - val_loss: 14869.4834 - val_accuracy: 0.0114\n",
      "Epoch 36/80\n",
      "625/625 [==============================] - 172s 275ms/step - loss: 14684.1074 - accuracy: 0.0111 - val_loss: 14468.0283 - val_accuracy: 0.0088\n",
      "Epoch 37/80\n",
      "625/625 [==============================] - 176s 281ms/step - loss: 14560.2959 - accuracy: 0.0107 - val_loss: 14644.3057 - val_accuracy: 0.0114\n",
      "Epoch 38/80\n",
      "126/625 [=====>........................] - ETA: 2:01 - loss: 14661.3760 - accuracy: 0.0114"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model.fit(x=train_ds, batch_size=batch_size, epochs=80, callbacks=callbacks, validation_data=val_ds, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "accf17c8cddd7598f858c901abd76bdd6a40d95d35cb16ec1025b995904db703"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
