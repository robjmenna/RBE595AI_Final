{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Conv1D, LSTM, MaxPool2D, Flatten, InputLayer, Reshape, TimeDistributed\n",
    "from tensorflow.keras.models import Sequential\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "data_path = \"E:\\\\595DeepLearning\\\\TermProject\\\\data_road\\\\training\"\n",
    "training_data_path = data_path + \"\\\\image_2\"\n",
    "label_data_path = data_path + \"\\\\gt_image_2\"\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        InputLayer(input_shape=(160,600,5)),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        MaxPool2D(),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        MaxPool2D(),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        MaxPool2D(),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        TimeDistributed(LSTM(64, return_sequences=True)),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        TimeDistributed(LSTM(64, return_sequences=True)),\n",
    "        Conv2D(64, (1,5), strides=(1,4), activation='relu'),\n",
    "        Conv2D(1, (1,4), activation='relu'),\n",
    "        Flatten(),\n",
    "        Dense(600, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def crop_image(image):\n",
    "    \"\"\"\n",
    "    Crop center 600x160 of the image and recale pixel values to [0,1]\n",
    "    \"\"\"\n",
    "    height, width, _ = image.shape\n",
    "    ymin = int(height/2-80)\n",
    "    ymax = int(height/2+80)\n",
    "    xmin = int(width/2-300)\n",
    "    xmax = int(width/2+300)\n",
    "    cropped = image[ymin:ymax, xmin:xmax]\n",
    "    cropped = cropped.astype('float32')\n",
    "    cropped /= 255.0\n",
    "\n",
    "    return cropped\n",
    "\n",
    "def resize_image(image):\n",
    "    \"\"\"\n",
    "    Resize image to 600x160 and recale pixel values to [0,1]\n",
    "    \"\"\"\n",
    "    resized = cv2.resize(image, dsize=(600, 160), interpolation=cv2.INTER_CUBIC)\n",
    "    resized = resized.astype('float32')\n",
    "    resized /= 255.0\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df = pd.DataFrame(columns=('image_path', 'label_path'))\n",
    "image_paths = os.listdir(training_data_path)\n",
    "\n",
    "for image_filename in image_paths:\n",
    "    x = image_filename.split(\"_\")\n",
    "    label_path = \"_\".join([x[0], \"road\", x[1]])\n",
    "    dict1 = {'image_path': image_filename, 'label_path': label_path}\n",
    "    names_df = names_df.append(dict1, ignore_index = True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20 of 384 labels\n",
      "Processed 40 of 384 labels\n",
      "Processed 60 of 384 labels\n",
      "Processed 80 of 384 labels\n",
      "Processed 100 of 384 labels\n",
      "Processed 120 of 384 labels\n",
      "Processed 140 of 384 labels\n",
      "Processed 160 of 384 labels\n",
      "Processed 180 of 384 labels\n",
      "Processed 200 of 384 labels\n",
      "Processed 220 of 384 labels\n",
      "Processed 240 of 384 labels\n",
      "Processed 260 of 384 labels\n",
      "Processed 280 of 384 labels\n"
     ]
    }
   ],
   "source": [
    "# Prepare training data\n",
    "\n",
    "# Get all filenames\n",
    "image_paths = os.listdir(training_data_path)\n",
    "\n",
    "plot_train_images = False\n",
    "resized_input_image_array = []\n",
    "cropped_input_image_array = []\n",
    "\n",
    "counter = 0\n",
    "for idx, values in names_df.iterrows():\n",
    "    image_filename = values.image_path\n",
    "    counter += 1\n",
    "    if counter % 20 == 0:\n",
    "        print(\"Processed {} of {} images\".format(counter, len(names_df.index)))\n",
    "    image_path = training_data_path + \"\\\\\" + image_filename\n",
    "\n",
    "    # Load image and create resized and cropped versions\n",
    "    image = cv2.imread(image_path)\n",
    "    resized = resize_image(image)\n",
    "    cropped = crop_image(image)\n",
    "\n",
    "    # Prepare the 4th and 5th dimensions which will be horizontal and verticles indices respectively\n",
    "    # Refer to https://arxiv.org/pdf/1808.04450.pdf section \"IV.B.1 Preprocessing\"\n",
    "    rows = np.ndarray([160,600])\n",
    "    for i in range(160):\n",
    "        rows[i, :] = i+1  * np.ones([600])\n",
    "    columns = np.ndarray([160,600])\n",
    "    for i in range(600):\n",
    "        columns[:, i] = i+1 * np.ones([160])\n",
    "    rows = np.expand_dims(rows, 2)\n",
    "    columns = np.expand_dims(columns, 2)\n",
    "\n",
    "    # Add row and column dimensions to our images to create 600x160x5 input tensors\n",
    "    image_tensor_resized = np.concatenate([resized, rows, columns], axis=2)\n",
    "    image_tensor_cropped = np.concatenate([cropped, rows, columns], axis=2)\n",
    "\n",
    "    # Plot images if boolean above is set to True\n",
    "    # Should only do this if processing a couple of images otherwise matplotlip won't be happy with YOU!\n",
    "    if plot_train_images:\n",
    "        fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "        ax1.imshow(image_tensor_resized[:,:,0:3])\n",
    "        ax1.set_title(\"resized\")\n",
    "        ax2.imshow(image_tensor_cropped[:,:,0:3])\n",
    "        ax2.set_title(\"cropped\")\n",
    "        \n",
    "    # Add the input tensors to the data arrays\n",
    "    resized_input_image_array.append(image_tensor_resized)\n",
    "    cropped_input_image_array.append(image_tensor_cropped)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20 of 384 labels\n",
      "Processed 40 of 384 labels\n",
      "Processed 60 of 384 labels\n",
      "Processed 80 of 384 labels\n",
      "Processed 100 of 384 labels\n",
      "Processed 120 of 384 labels\n",
      "Processed 140 of 384 labels\n",
      "Processed 160 of 384 labels\n",
      "Processed 180 of 384 labels\n",
      "Processed 200 of 384 labels\n",
      "Processed 220 of 384 labels\n",
      "Processed 240 of 384 labels\n",
      "Processed 260 of 384 labels\n",
      "Processed 280 of 384 labels\n"
     ]
    }
   ],
   "source": [
    "# Prepare training labels\n",
    "\n",
    "# Get all filenames\n",
    "label_paths = os.listdir(label_data_path)\n",
    "\n",
    "plot_label_images = False\n",
    "resized_label_array = []\n",
    "cropped_label_array = []\n",
    "\n",
    "# Loop through all images in the labeled picture directory\n",
    "counter = 0\n",
    "for idx, values in names_df.iterrows():\n",
    "    image_filename = values.label_path\n",
    "#     print(label_image)\n",
    "#     print(\"road\" not in label_image)\n",
    "#     if \"road\" not in label_image:\n",
    "#         print(\"skipping\")\n",
    "#         continue\n",
    "    counter += 1\n",
    "    if counter % 20 == 0:\n",
    "        print(\"Processed {} of {} labels\".format(counter, len(names_df.index)))\n",
    "    \n",
    "    # Load the image and create a resized and cropped version\n",
    "    image_path = label_data_path + \"\\\\\" + label_image\n",
    "    image = cv2.imread(image_path)\n",
    "    resized = resize_image(image)\n",
    "    cropped = crop_image(image)\n",
    "\n",
    "    # Initialize the 600x1x1 ndarray to be used as the label for this data point\n",
    "    # The value is the height of the column where the road borders non-road\n",
    "    resized_label_img = np.ndarray([600])\n",
    "    cropped_label_img = np.ndarray([600])\n",
    "\n",
    "    # Loop through each column of the image to see where the road segmentation begins in the label\n",
    "    for i in range(resized.shape[1]):\n",
    "        # Check the through the resized image to see where the segmentation begins\n",
    "        for j in range(resized.shape[0]):\n",
    "            if np.array_equiv(resized[j,i],[1.,0.,1.]):\n",
    "                break\n",
    "        resized_label_img[i] = j+1\n",
    "        \n",
    "        # Check through the cropped image to see where the segmentation begins\n",
    "        for j in range(resized.shape[0]):\n",
    "            if np.array_equiv(cropped[j,i],[1.,0.,1.]):\n",
    "                break\n",
    "        cropped_label_img[i] = j+1\n",
    "\n",
    "    # Expand dimensions to turn the 600 to a 600x1x1\n",
    "    resized_label_img = np.expand_dims(resized_label_img, 1)\n",
    "    resized_label_img = np.expand_dims(resized_label_img, 2)\n",
    "    cropped_label_img = np.expand_dims(cropped_label_img, 1)\n",
    "    cropped_label_img = np.expand_dims(cropped_label_img, 2)\n",
    "\n",
    "    # Add the 600x1x1 label to the label array\n",
    "    resized_label_array.append(resized_label_img)\n",
    "    cropped_label_array.append(cropped_label_img)\n",
    "    \n",
    "    # Plot images if boolean above is set to True\n",
    "    # Should only do this if processing a couple of images otherwise matplotlip won't be happy with YOU, yes YOU!!\n",
    "    if plot_label_images:\n",
    "        fig, (ax1) = plt.subplots(1,2)\n",
    "        ax1.imshow(resized)\n",
    "        ax1.set_title(\"resized label\")\n",
    "        ax2.imshow(cropped)\n",
    "        ax2.set_title(\"cropped label\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data and label arrays to nparrays before feeding it to the models\n",
    "resized_input_image_array_np = np.array(resized_input_image_array)\n",
    "resized_input_image_array_np = resized_input_image_array_np.astype('float32')\n",
    "cropped_input_image_array_np = np.array(cropped_input_image_array)\n",
    "cropped_input_image_array_np = cropped_input_image_array_np.astype('float32')\n",
    "resized_label_array_np = np.array(resized_label_array)\n",
    "cropped_label_array_np = np.array(cropped_label_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the models\n",
    "resized_model = create_model()\n",
    "\n",
    "# Check dem summaries\n",
    "# resized_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Zeke\\anaconda3\\envs\\newenvironmentname\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 289 samples\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Cannot update variable with shape [] using a Tensor with shape [600,32], shapes must be equal.\n\t [[{{node metrics_24/acc/AssignAddVariableOp}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-174-77d02b2a559a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Train dem models'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mresized_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresized_input_image_array_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresized_label_array_np\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\newenvironmentname\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\newenvironmentname\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\newenvironmentname\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\newenvironmentname\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3476\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\newenvironmentname\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Cannot update variable with shape [] using a Tensor with shape [600,32], shapes must be equal.\n\t [[{{node metrics_24/acc/AssignAddVariableOp}}]]"
     ]
    }
   ],
   "source": [
    "# Train dem models'\n",
    "resized_model.fit(resized_input_image_array_np, resized_label_array_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the models\n",
    "cropped_model = create_model()\n",
    "\n",
    "# Check dem summaries\n",
    "# cropped_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dem models\n",
    "\n",
    "cropped_model.fit(cropped_input_image_array_np, cropped_label_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a132e1c5f1b31d5e927d688f965514e030dfbafb3124753db69ec43e5ee86536"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
