{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:595DeepLearning\\TermProject\\data_road\\training\\image_2\n",
      "E:595DeepLearning\\TermProject\\data_road\\training\\gt_image_2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Conv1D, LSTM, MaxPool2D, Flatten, InputLayer, Reshape, TimeDistributed\n",
    "from tensorflow.keras.models import Sequential\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# data_path = os.path.join('data_road','training')\n",
    "data_path = os.path.join('E:', '595DeepLearning', 'TermProject', 'data_road','training')\n",
    "training_data_path = os.path.join(data_path, \"image_2\")\n",
    "label_data_path = os.path.join(data_path, \"gt_image_2\")\n",
    "print(training_data_path)\n",
    "print(label_data_path)\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        InputLayer(input_shape=(160,600,5)),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        MaxPool2D(),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        MaxPool2D(),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        MaxPool2D(),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        TimeDistributed(LSTM(64, return_sequences=True)),\n",
    "        Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "        TimeDistributed(LSTM(64, return_sequences=True)),\n",
    "        Conv2D(64, (1,5), strides=(1,4), activation='relu'),\n",
    "        Conv2D(1, (1,4), activation='relu'),\n",
    "        Flatten(),\n",
    "        Dense(600, activation='linear')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def crop_image(image):\n",
    "    \"\"\"\n",
    "    Crop center 600x160 of the image and recale pixel values to [0,1]\n",
    "    \"\"\"\n",
    "    height, width, _ = image.shape\n",
    "    ymin = int(height/2-80)\n",
    "    ymax = int(height/2+80)\n",
    "    xmin = int(width/2-300)\n",
    "    xmax = int(width/2+300)\n",
    "    cropped = image[ymin:ymax, xmin:xmax]\n",
    "    return cropped\n",
    "\n",
    "def resize_image(image):\n",
    "    \"\"\"\n",
    "    Resize image to 600x160 and recale pixel values to [0,1]\n",
    "    \"\"\"\n",
    "    resized = cv2.resize(image, dsize=(600, 160), interpolation=cv2.INTER_CUBIC)\n",
    "    return resized\n",
    "\n",
    "def process_and_normalize_image(image):\n",
    "    image = image.astype('float32')\n",
    "    image /= 255.0\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df = pd.DataFrame(columns=('image_path', 'label_path'))\n",
    "image_paths = os.listdir(training_data_path)\n",
    "\n",
    "for image_filename in image_paths:\n",
    "    x = image_filename.split(\"_\")\n",
    "    label_path = \"_\".join([x[0], \"road\", x[1]])\n",
    "    dict1 = {'image_path': image_filename, 'label_path': label_path}\n",
    "    names_df = names_df.append(dict1, ignore_index = True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20 of 289 images\n",
      "Processed 40 of 289 images\n",
      "Processed 60 of 289 images\n",
      "Processed 80 of 289 images\n",
      "Processed 100 of 289 images\n",
      "Processed 120 of 289 images\n",
      "Processed 140 of 289 images\n",
      "Processed 160 of 289 images\n",
      "Processed 180 of 289 images\n",
      "Processed 200 of 289 images\n",
      "Processed 220 of 289 images\n",
      "Processed 240 of 289 images\n",
      "Processed 260 of 289 images\n",
      "Processed 280 of 289 images\n"
     ]
    }
   ],
   "source": [
    "# Prepare training data\n",
    "\n",
    "# Get all filenames\n",
    "image_paths = os.listdir(training_data_path)\n",
    "\n",
    "plot_train_images = False\n",
    "resized_input_image_array = []\n",
    "cropped_input_image_array = []\n",
    "\n",
    "counter = 0\n",
    "for idx, values in names_df.iterrows():\n",
    "    image_filename = values.image_path\n",
    "    counter += 1\n",
    "    if counter % 20 == 0:\n",
    "        print(\"Processed {} of {} images\".format(counter, len(names_df.index)))\n",
    "    image_path = os.path.join(training_data_path, image_filename)\n",
    "\n",
    "    # Load image and create resized and cropped versions\n",
    "    image = cv2.imread(image_path)\n",
    "    resized_imshow = resize_image(image)\n",
    "    resized = process_and_normalize_image(resized_imshow)\n",
    "    cropped_imshow = crop_image(image)\n",
    "    cropped = process_and_normalize_image(cropped_imshow)\n",
    "\n",
    "    # Prepare the 4th and 5th dimensions which will be horizontal and verticles indices respectively\n",
    "    # Refer to https://arxiv.org/pdf/1808.04450.pdf section \"IV.B.1 Preprocessing\"\n",
    "    rows = np.ndarray([160,600])\n",
    "    for i in range(160):\n",
    "        rows[i, :] = i+1  * np.ones([600])\n",
    "    columns = np.ndarray([160,600])\n",
    "    for i in range(600):\n",
    "        columns[:, i] = i+1 * np.ones([160])\n",
    "    rows = np.expand_dims(rows, 2)\n",
    "    columns = np.expand_dims(columns, 2)\n",
    "\n",
    "    # Add row and column dimensions to our images to create 600x160x5 input tensors\n",
    "    image_tensor_resized = np.concatenate([resized, rows, columns], axis=2)\n",
    "    image_tensor_cropped = np.concatenate([cropped, rows, columns], axis=2)\n",
    "\n",
    "    # Plot images if boolean above is set to True\n",
    "    # Should only do this if processing a couple of images otherwise matplotlip won't be happy with YOU!\n",
    "    if plot_train_images:\n",
    "        fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "        ax1.imshow(image_tensor_resized[:,:,0:3])\n",
    "        ax1.set_title(\"resized\")\n",
    "        ax2.imshow(image_tensor_cropped[:,:,0:3])\n",
    "        ax2.set_title(\"cropped\")\n",
    "        \n",
    "    # Add the input tensors to the data arrays\n",
    "    resized_input_image_array.append(image_tensor_resized)\n",
    "    cropped_input_image_array.append(image_tensor_cropped)\n",
    "print(\"Processed {} of {} labels\".format(counter, len(names_df.index)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20 of 289 labels\n",
      "Processed 40 of 289 labels\n",
      "Processed 60 of 289 labels\n",
      "Processed 80 of 289 labels\n",
      "Processed 100 of 289 labels\n",
      "Processed 120 of 289 labels\n",
      "Processed 140 of 289 labels\n",
      "Processed 160 of 289 labels\n",
      "Processed 180 of 289 labels\n",
      "Processed 200 of 289 labels\n",
      "Processed 220 of 289 labels\n",
      "Processed 240 of 289 labels\n",
      "Processed 260 of 289 labels\n",
      "Processed 280 of 289 labels\n",
      "Processed 289 of 289 labels\n"
     ]
    }
   ],
   "source": [
    "# Prepare training labels\n",
    "\n",
    "# Get all filenames\n",
    "label_paths = os.listdir(label_data_path)\n",
    "\n",
    "plot_label_images = False\n",
    "resized_label_array = []\n",
    "cropped_label_array = []\n",
    "\n",
    "# Loop through all images in the labeled picture directory\n",
    "counter = 0\n",
    "for idx, values in names_df.iterrows():\n",
    "    image_filename = values.label_path\n",
    "#     print(label_image)\n",
    "#     print(\"road\" not in label_image)\n",
    "#     if \"road\" not in label_image:\n",
    "#         print(\"skipping\")\n",
    "#         continue\n",
    "    counter += 1\n",
    "    if counter % 20 == 0:\n",
    "        print(\"Processed {} of {} labels\".format(counter, len(names_df.index)))\n",
    "    \n",
    "    # Load the image and create a resized and cropped version\n",
    "    image_path = os.path.join(label_data_path, image_filename)\n",
    "    image = cv2.imread(image_path)\n",
    "    resized_imshow = resize_image(image)\n",
    "    resized = process_and_normalize_image(resized_imshow)\n",
    "    cropped_imshow = crop_image(image)\n",
    "    cropped = process_and_normalize_image(cropped_imshow)\n",
    "\n",
    "    # Initialize the 600x1x1 ndarray to be used as the label for this data point\n",
    "    # The value is the height of the column where the road borders non-road\n",
    "    resized_label_img = np.ndarray([600])\n",
    "    cropped_label_img = np.ndarray([600])\n",
    "\n",
    "    # Loop through each column of the image to see where the road segmentation begins in the label\n",
    "    for i in range(resized.shape[1]):\n",
    "        # Check the through the resized image to see where the segmentation begins\n",
    "        for j in range(resized.shape[0]):\n",
    "            if np.array_equiv(resized[j,i],[1.,0.,1.]):\n",
    "                break\n",
    "        resized_label_img[i] = j+1\n",
    "        \n",
    "        # Check through the cropped image to see where the segmentation begins\n",
    "        for j in range(resized.shape[0]):\n",
    "            if np.array_equiv(cropped[j,i],[1.,0.,1.]):\n",
    "                break\n",
    "        cropped_label_img[i] = j+1\n",
    "\n",
    "    # Expand dimensions to turn the 600 to a 600x1x1\n",
    "#     resized_label_img = np.expand_dims(resized_label_img, 1)\n",
    "#     resized_label_img = np.expand_dims(resized_label_img, 2)\n",
    "#     cropped_label_img = np.expand_dims(cropped_label_img, 1)\n",
    "#     cropped_label_img = np.expand_dims(cropped_label_img, 2)\n",
    "\n",
    "    # Add the 600x1x1 label to the label array\n",
    "    resized_label_array.append(resized_label_img)\n",
    "    cropped_label_array.append(cropped_label_img)\n",
    "    \n",
    "    # Plot images if boolean above is set to True\n",
    "    # Should only do this if processing a couple of images otherwise matplotlip won't be happy with YOU, yes YOU!!\n",
    "    if plot_label_images:\n",
    "        fig, (ax1) = plt.subplots(1,2)\n",
    "        ax1.imshow(resized)\n",
    "        ax1.set_title(\"resized label\")\n",
    "        ax2.imshow(cropped)\n",
    "        ax2.set_title(\"cropped label\")\n",
    "print(\"Processed {} of {} labels\".format(counter, len(names_df.index)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(289, 160, 600, 5)\n",
      "(289, 160, 600, 5)\n",
      "(289, 600)\n",
      "(289, 600, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Convert data and label arrays to nparrays before feeding it to the models\n",
    "resized_input_image_array_np = np.array(resized_input_image_array)\n",
    "resized_input_image_array_np = resized_input_image_array_np.astype('float32')\n",
    "cropped_input_image_array_np = np.array(cropped_input_image_array)\n",
    "cropped_input_image_array_np = cropped_input_image_array_np.astype('float32')\n",
    "resized_label_array_np = np.array(resized_label_array)\n",
    "cropped_label_array_np = np.array(cropped_label_array).reshape((289,600,1,1))\n",
    "print(resized_input_image_array_np.shape)\n",
    "print(cropped_input_image_array_np.shape)\n",
    "print(resized_label_array_np.shape)\n",
    "print(cropped_label_array_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the models\n",
    "tf.keras.backend.clear_session()\n",
    "resized_model = create_model()\n",
    "\n",
    "# Check dem summaries\n",
    "# resized_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 289 samples\n",
      "Epoch 1/3\n",
      "289/289 [==============================] - 105s 364ms/sample - loss: 18068.1471 - acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "289/289 [==============================] - 104s 359ms/sample - loss: 18065.5130 - acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "289/289 [==============================] - 113s 392ms/sample - loss: 18062.8675 - acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# Train dem models'\n",
    "# resized_model = load_model('resized_model.h5')\n",
    "resized_model.fit(resized_input_image_array_np, resized_label_array_np, epochs=3)\n",
    "# resized_model.fit(resized_input_image_array_np, resized_label_array_np)\n",
    "resized_model.save(\"resized_model_{}.h5\".format(datetime.utcnow().strftime(\"%Y_%m_%d_%H_%M_%S\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the models\n",
    "tf.keras.backend.clear_session()\n",
    "cropped_model = create_model()\n",
    "\n",
    "# Check dem summaries\n",
    "# cropped_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dem models\n",
    "# cropped_model = load_model('resized_model.h5')\n",
    "cropped_model.fit(cropped_input_image_array_np, cropped_label_array_np)\n",
    "cropped_model.save(\"cropped_model_{}.h5\".format(datetime.utcnow().strftime(\"%Y_%m_%d_%H_%M_%S\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a132e1c5f1b31d5e927d688f965514e030dfbafb3124753db69ec43e5ee86536"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
